{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', u'spark.dynamicAllocation.enabled': False, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 20.65087890625, "end_time": 1618778772700.939}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "/**\n * @Description: a spatial join based on Filter-refine approach for NYC taxicab data\n * @author: Isam Al Jawarneh\n * @date: 02/02/2019\n *last update: 14/04/2021\n */", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>18</td><td>application_1618739841902_0022</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sspark.2qmuuk3tadvermeauflhohlawa.fx.internal.cloudapp.net:8088/proxy/application_1618739841902_0022/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-sspark.2qmuuk3tadvermeauflhohlawa.fx.internal.cloudapp.net:30060/node/containerlogs/container_1618739841902_0022_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 760.56689453125, "end_time": 1618778805495.791}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "sc.version", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res3: String = 2.2.0.2.6.3.84-1"}], "metadata": {"cell_status": {"execute_time": {"duration": 761.44189453125, "end_time": 1618778806268.017}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.DataFrame"}], "metadata": {"cell_status": {"execute_time": {"duration": 7315.43798828125, "end_time": 1618778813597.152}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "/////////////////////////////\n/// Definition of schemas ///\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 759.216064453125, "end_time": 1618778814367.475}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "val parametersRegistrySchema = StructType(Array(\n    StructField(\"IdParametro\", StringType, false),\n    StructField(\"PARAMETRO\", StringType, false),\n    StructField(\"UM\", StringType, false),\n    StructField(\"Tmed (min)\", DoubleType, false),\n    StructField(\"NOTE\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "parametersRegistrySchema: org.apache.spark.sql.types.StructType = StructType(StructField(IdParametro,StringType,false), StructField(PARAMETRO,StringType,false), StructField(UM,StringType,false), StructField(Tmed (min),DoubleType,false), StructField(NOTE,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 1272.075927734375, "end_time": 1618778815651.184}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 7, "cell_type": "code", "source": "val stationsRegistrySchema = StructType(Array(\n    StructField(\"Stazione\", StringType, false),\n    StructField(\"Cod_staz\", StringType, false),\n    StructField(\"COMUNE\", StringType, false),\n    StructField(\"INDIRIZZO\", StringType, false),\n    StructField(\"PROVINCIA\", StringType, false),\n    StructField(\"Altezza\", StringType, false),\n    StructField(\"Id_Param\", StringType, false),\n    StructField(\"PARAMETRO\", StringType, false),\n    StructField(\"UM\", StringType, false),\n    StructField(\"Coord_X\", DoubleType, false),\n    StructField(\"Coord_Y\", DoubleType, false),\n    StructField(\"SR\", StringType, false),\n    StructField(\"LON_GEO\", DoubleType, false),\n    StructField(\"LAT_GEO\", DoubleType, false),\n    StructField(\"SR_GEO\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "stationsRegistrySchema: org.apache.spark.sql.types.StructType = StructType(StructField(Stazione,StringType,false), StructField(Cod_staz,StringType,false), StructField(COMUNE,StringType,false), StructField(INDIRIZZO,StringType,false), StructField(PROVINCIA,StringType,false), StructField(Altezza,StringType,false), StructField(Id_Param,StringType,false), StructField(PARAMETRO,StringType,false), StructField(UM,StringType,false), StructField(Coord_X,DoubleType,false), StructField(Coord_Y,DoubleType,false), StructField(SR,StringType,false), StructField(LON_GEO,DoubleType,false), StructField(LAT_GEO,DoubleType,false), StructField(SR_GEO,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 759.2900390625, "end_time": 1618778816422.663}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 8, "cell_type": "code", "source": "val airDataSchema = StructType(Array(\n    StructField(\"COD_STAZ\", StringType, false),\n    StructField(\"ID_PARAM\", StringType, false),\n    StructField(\"DATA_INIZIO\", StringType, false),\n    StructField(\"DATA_FINE\", StringType, false),\n    StructField(\"VALORE\", DoubleType, false),\n    StructField(\"UM\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "airDataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(COD_STAZ,StringType,false), StructField(ID_PARAM,StringType,false), StructField(DATA_INIZIO,StringType,false), StructField(DATA_FINE,StringType,false), StructField(VALORE,DoubleType,false), StructField(UM,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 770.76611328125, "end_time": 1618778817205.168}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 9, "cell_type": "code", "source": "/////////////////////////////\n///// Import Dataframes /////\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 758.2099609375, "end_time": 1618778817973.832}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval parametersRegistry = (spark.read.format(\"csv\")\n                          .option(\"header\", \"true\")\n                          .schema(parametersRegistrySchema)\n                          .csv(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/registries/Parametri.csv\")\n                          .select($\"IdParametro\".as(\"Parameter_Id\"),\n                                  $\"PARAMETRO\".as(\"Parameter_Name\"),\n                                  $\"UM\".as(\"Unit_Of_Measurement\"),\n                                  $\"Tmed (min)\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "parametersRegistry: org.apache.spark.sql.DataFrame = [Parameter_Id: string, Parameter_Name: string ... 2 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 5294.93505859375, "end_time": 1618778823281.869}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 11, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval stationsRegistry = (spark.read.format(\"csv\")\n                        .option(\"header\", \"true\")\n                        .schema(stationsRegistrySchema)\n                        .csv(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/registries/Stazioni Aria.csv\")\n                        .select($\"Stazione\".as(\"Station\"), $\"Cod_staz\", $\"COMUNE\".as(\"City\"), $\"INDIRIZZO\".as(\"Address\"), $\"PROVINCIA\".as(\"Province\"), $\"LON_GEO\", $\"LAT_GEO\")\n                        .withColumn(\"Station_Code\", regexp_replace($\"Cod_staz\", \"\\\\.\", \"\"))\n                        .withColumn(\"Point\", point($\"LON_GEO\",$\"LAT_GEO\"))\n                        .drop(\"Cod_Staz\", \"LON_GEO\", \"LAT_GEO\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "stationsRegistry: org.apache.spark.sql.DataFrame = [Station: string, City: string ... 4 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 1271.244140625, "end_time": 1618778824565.464}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 12, "cell_type": "code", "source": "val airData = (spark.read.format(\"csv\")\n               .option(\"header\", \"true\")\n               .schema(airDataSchema)\n               .load(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/datasets_air_quality/\")\n               .select($\"COD_STAZ\".as(\"Station_Code\"), $\"ID_PARAM\".as(\"Parameter_Id\"), $\"DATA_INIZIO\", $\"DATA_FINE\", $\"VALORE\".as(\"Value\"))\n               .withColumn(\"Start_Date\", to_timestamp($\"DATA_INIZIO\", \"dd/MM/yyyy HH\"))\n               .withColumn(\"End_Date\", to_timestamp($\"DATA_FINE\", \"dd/MM/yyyy HH\"))\n               .drop(\"DATA_INIZIO\", \"DATA_FINE\")\n              )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "airData: org.apache.spark.sql.DataFrame = [Station_Code: string, Parameter_Id: string ... 3 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 1264.09912109375, "end_time": 1618778825841.549}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 13, "cell_type": "code", "source": "//////////////////\n/// Geohashing ///\n//////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.89111328125, "end_time": 1618778826612.775}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 14, "cell_type": "code", "source": "// a user defined function to get geohash from long/lat point \nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@17ef6849,true))))"}], "metadata": {"cell_status": {"execute_time": {"duration": 1274.583984375, "end_time": 1618778827899.341}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 15, "cell_type": "code", "source": "val precision = 30", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision: Int = 30"}], "metadata": {"cell_status": {"execute_time": {"duration": 754.81005859375, "end_time": 1618778828665.451}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 16, "cell_type": "code", "source": "//getting plain data from CSV file (file with point Data Structure) and use UDF to get geohashes\nval geohashedStations = (stationsRegistry\n                         .withColumn(\"index\", $\"point\" index  precision)\n                         .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedStations = (geohashedStations\n                                 .explode(\"geohashArray1\", \"geohash\")\n                                 { a: mutable.WrappedArray[String] => a })", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedGeohashedStations: org.apache.spark.sql.DataFrame = [Station: string, City: string ... 7 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 1321.327880859375, "end_time": 1618778829998.87}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 17, "cell_type": "code", "source": "explodedGeohashedStations.select(\"Station\", \"Point\", \"index\", \"geohashArray1\", \"geohash\").show(2,false)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|Station            |Point                                      |index                                                                                                                                                 |geohashArray1|geohash|\n+-------------------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|GIARDINI MARGHERITA|Point(11.35406170088398, 44.48267113876953)|[[ZOrderCurve(11.348876953125, 44.4781494140625, 11.35986328125, 44.483642578125, 30, -4191424035449470976, 110001011101010100010010000011),Contains]]|[srbj43]     |srbj43 |\n|GIARDINI MARGHERITA|Point(11.35406170088398, 44.48267113876953)|[[ZOrderCurve(11.348876953125, 44.4781494140625, 11.35986328125, 44.483642578125, 30, -4191424035449470976, 110001011101010100010010000011),Contains]]|[srbj43]     |srbj43 |\n+-------------------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 5307.85595703125, "end_time": 1618778835317.782}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 18, "cell_type": "code", "source": "val rawEmiliaRomagna= (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/bologna/\")\n                  .select($\"polygon\", $\"metadata\"(\"NOME_COM\").as(\"City_Name\")).cache()\n                  )\nval emiliaRomagna = (rawEmiliaRomagna\n               .withColumn(\"index\", $\"polygon\" index  precision)\n               .select($\"polygon\", $\"index\", $\"City_Name\")\n               .cache())\nval zorderIndexedEmiliaRomagna = (emiliaRomagna\n                            .withColumn(\"index\", explode($\"index\"))\n                            .select(\"polygon\", \"index.curve\", \"index.relation\",\"City_Name\")\n                          )\nval geohashedEmiliaRomagna = emiliaRomagna.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedGeohashedEmiliaRomagna = geohashedEmiliaRomagna.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\nexplodedGeohashedEmiliaRomagna.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res22: Long = 54674"}], "metadata": {"cell_status": {"execute_time": {"duration": 5308.705810546875, "end_time": 1618778840640.972}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 19, "cell_type": "code", "source": "//joining geohashed trips with exploded geohashed neighborhood using filter-and-refine approach (.where($\"point\" within $\"polygon\") is refine --> using the brute force method ray casting for edge cases or false positives)\nval stationsInEmiliaRomagna = (explodedGeohashedEmiliaRomagna\n                         .join(explodedGeohashedStations,\n                               explodedGeohashedEmiliaRomagna(\"geohash\") === explodedGeohashedStations(\"geohash\"))\n                         .where($\"point\" within $\"polygon\")\n                        )\nstationsInEmiliaRomagna.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+--------------------+--------------------+-------+---------+--------------------+-----------+--------+------------+--------------------+--------------------+-------------+-------+\n|             polygon|               index|           City_Name|        geohashArray|geohash|  Station|                City|    Address|Province|Station_Code|               Point|               index|geohashArray1|geohash|\n+--------------------+--------------------+--------------------+--------------------+-------+---------+--------------------+-----------+--------+------------+--------------------+--------------------+-------------+-------+\n|magellan.Polygon@...|[[ZOrderCurve(9.6...|Lugagnano Val d'Arda|[spyysw, spyysx, ...| spyyzx|LUGAGNANO|LUGAGNANO VAL D'ARDA|VIA FERMI 9|      PC|     5000007|Point(9.829360661...|[[ZOrderCurve(9.8...|     [spyyzx]| spyyzx|\n|magellan.Polygon@...|[[ZOrderCurve(9.6...|Lugagnano Val d'Arda|[spyysw, spyysx, ...| spyyzx|LUGAGNANO|LUGAGNANO VAL D'ARDA|VIA FERMI 9|      PC|     5000007|Point(9.829360661...|[[ZOrderCurve(9.8...|     [spyyzx]| spyyzx|\n|magellan.Polygon@...|[[ZOrderCurve(9.6...|Lugagnano Val d'Arda|[spyysw, spyysx, ...| spyyzx|LUGAGNANO|LUGAGNANO VAL D'ARDA|VIA FERMI 9|      PC|     5000007|Point(9.829360661...|[[ZOrderCurve(9.8...|     [spyyzx]| spyyzx|\n+--------------------+--------------------+--------------------+--------------------+-------+---------+--------------------+-----------+--------+------------+--------------------+--------------------+-------------+-------+\nonly showing top 3 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 2277.087158203125, "end_time": 1618778842930.201}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 20, "cell_type": "code", "source": "stationsInEmiliaRomagna.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res25: Array[String] = Array(polygon, index, City_Name, geohashArray, geohash, Station, City, Address, Province, Station_Code, Point, index, geohashArray1, geohash)"}], "metadata": {"cell_status": {"execute_time": {"duration": 757.93896484375, "end_time": 1618778843699.442}}, "collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "val airDataWithParameters = (airData\n                             .join(\n                                 parametersRegistry,\n                                 airData(\"Parameter_Id\") === parametersRegistry(\"Parameter_Id\")))\nairDataWithParameters.show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+------------+-----+-------------------+-------------------+------------+--------------------+-------------------+----------+\n|Station_Code|Parameter_Id|Value|         Start_Date|           End_Date|Parameter_Id|      Parameter_Name|Unit_Of_Measurement|Tmed (min)|\n+------------+------------+-----+-------------------+-------------------+------------+--------------------+-------------------+----------+\n|     5000033|          10|  1.2|2020-01-01 00:00:00|2020-01-01 01:00:00|          10|CO (Monossido di ...|              mg/m3|      60.0|\n|     5000033|          10|  1.0|2020-01-01 01:00:00|2020-01-01 02:00:00|          10|CO (Monossido di ...|              mg/m3|      60.0|\n+------------+------------+-----+-------------------+-------------------+------------+--------------------+-------------------+----------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 2271.432861328125, "end_time": 1618778845983.902}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 22, "cell_type": "code", "source": "val finalTable = (airDataWithParameters\n                  .join(stationsInEmiliaRomagna,\n                        airDataWithParameters(\"Station_Code\") === stationsInEmiliaRomagna(\"Station_Code\")))\nfinalTable.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+------------+-----+-------------------+-------------------+------------+--------------------+-------------------+----------+--------------------+--------------------+---------+--------------------+-------+---------+-----+---------------+--------+------------+--------------------+--------------------+-------------+-------+\n|Station_Code|Parameter_Id|Value|         Start_Date|           End_Date|Parameter_Id|      Parameter_Name|Unit_Of_Measurement|Tmed (min)|             polygon|               index|City_Name|        geohashArray|geohash|  Station| City|        Address|Province|Station_Code|               Point|               index|geohashArray1|geohash|\n+------------+------------+-----+-------------------+-------------------+------------+--------------------+-------------------+----------+--------------------+--------------------+---------+--------------------+-------+---------+-----+---------------+--------+------------+--------------------+--------------------+-------------+-------+\n|     7000002|           8| 26.0|2020-01-01 00:00:00|2020-01-01 01:00:00|           8|NO2 (Biossido di ...|              ug/m3|      60.0|magellan.Polygon@...|[[ZOrderCurve(11....|    Imola|[srbhpe, srbhps, ...| srbk6d|DE AMICIS|IMOLA|VIALE DE AMICIS|      BO|     7000002|Point(11.71971103...|[[ZOrderCurve(11....|     [srbk6d]| srbk6d|\n|     7000002|           8| 26.0|2020-01-01 00:00:00|2020-01-01 01:00:00|           8|NO2 (Biossido di ...|              ug/m3|      60.0|magellan.Polygon@...|[[ZOrderCurve(11....|    Imola|[srbhpe, srbhps, ...| srbk6d|DE AMICIS|IMOLA|VIALE DE AMICIS|      BO|     7000002|Point(11.71971103...|[[ZOrderCurve(11....|     [srbk6d]| srbk6d|\n|     7000002|           8| 26.0|2020-01-01 00:00:00|2020-01-01 01:00:00|           8|NO2 (Biossido di ...|              ug/m3|      60.0|magellan.Polygon@...|[[ZOrderCurve(11....|    Imola|[srbhpe, srbhps, ...| srbk6d|DE AMICIS|IMOLA|VIALE DE AMICIS|      BO|     7000002|Point(11.71971103...|[[ZOrderCurve(11....|     [srbk6d]| srbk6d|\n+------------+------------+-----+-------------------+-------------------+------------+--------------------+-------------------+----------+--------------------+--------------------+---------+--------------------+-------+---------+-----+---------------+--------+------------+--------------------+--------------------+-------------+-------+\nonly showing top 3 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 9318.9619140625, "end_time": 1618778855315.485}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 23, "cell_type": "code", "source": "finalTable.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Station_Code: string (nullable = true)\n |-- Parameter_Id: string (nullable = true)\n |-- Value: double (nullable = true)\n |-- Start_Date: timestamp (nullable = true)\n |-- End_Date: timestamp (nullable = true)\n |-- Parameter_Id: string (nullable = true)\n |-- Parameter_Name: string (nullable = true)\n |-- Unit_Of_Measurement: string (nullable = true)\n |-- Tmed (min): double (nullable = true)\n |-- polygon: polygon (nullable = true)\n |-- index: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- curve: zordercurve (nullable = false)\n |    |    |-- relation: string (nullable = false)\n |-- City_Name: string (nullable = true)\n |-- geohashArray: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geohash: string (nullable = true)\n |-- Station: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Address: string (nullable = true)\n |-- Province: string (nullable = true)\n |-- Station_Code: string (nullable = true)\n |-- Point: point (nullable = false)\n |-- index: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- curve: zordercurve (nullable = false)\n |    |    |-- relation: string (nullable = false)\n |-- geohashArray1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geohash: string (nullable = true)"}], "metadata": {"cell_status": {"execute_time": {"duration": 753.929931640625, "end_time": 1618778856082.027}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}