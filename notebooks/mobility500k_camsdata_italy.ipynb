{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": true,\n        \"spark.shuffle.service.enabled\": true\n    }\n}\n", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', u'spark.dynamicAllocation.enabled': True, u'spark.shuffle.service.enabled': True, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 19.4609375, "end_time": 1619538913621.74}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1619521572498_0009</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-denis.oakn5vh0d33u3cgh5twveiaz3a.fx.internal.cloudapp.net:8088/proxy/application_1619521572498_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-denis.oakn5vh0d33u3cgh5twveiaz3a.fx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1619521572498_0009_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nimport org.apache.spark.sql.DataFrame"}], "metadata": {"cell_status": {"execute_time": {"duration": 9332.65478515625, "end_time": 1619538957382.847}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "val containerStorageName = \"denis-2021-04-27t09-50-33-745z\"\nval storageAccountName = \"denishdistorage\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "storageAccountName: String = denishdistorage"}], "metadata": {"cell_status": {"execute_time": {"duration": 752.804931640625, "end_time": 1619538958146.683}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "/////////////////////////////\n/// Definition of schemas ///\n/////////////////////////////\n\nval aerosolDataSchema = StructType(Array(\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"dataDate\", StringType, false),\n    StructField(\"time\", StringType, false),\n    StructField(\"shortName\", StringType, false)))\n\nval mobilityDataSchema = StructType(Array(\n    StructField(\"Code\", StringType, false),\n    StructField(\"Timestamp\", StringType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Status\", StringType, false),\n    StructField(\"Other_Value\", DoubleType, false),\n    StructField(\"Other_Date\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "mobilityDataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Code,StringType,false), StructField(Timestamp,StringType,false), StructField(Value,DoubleType,false), StructField(Latitude,DoubleType,false), StructField(Longitude,DoubleType,false), StructField(Status,StringType,false), StructField(Other_Value,DoubleType,false), StructField(Other_Date,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 2379.52392578125, "end_time": 1619538960538.404}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 83, "cell_type": "code", "source": "/////////////////////////////\n///// Import Dataframes /////\n/////////////////////////////\n\n//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval aerosolData = (spark.read.format(\"csv\")\n                        .option(\"header\", \"true\")\n                        .schema(aerosolDataSchema)\n                        .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/cams_air_data/cams_data_italy_right_dates.csv\")\n                        .withColumn(\"Timestamp\", to_timestamp(concat($\"dataDate\", lit(\" \"), $\"time\"), \"yyyyMMdd HHmm\"))\n                        .withColumn(\"Point\", point($\"Longitude\",$\"Latitude\"))\n                        .drop(\"dataDate\", \"time\"))\n\nval mobilityData = (spark.read.format(\"csv\")\n                    .option(\"header\", \"true\")\n                    .option(\"delimiter\", \";\")\n                    .schema(mobilityDataSchema)\n                    .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/mobility_data/db500mila.csv\")\n                    .withColumn(\"Timestamp\", to_timestamp($\"Timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n                    .withColumn(\"point\", point($\"Longitude\",$\"Latitude\"))\n                    .drop(\"Status\", \"Other_Value\", \"Other_Date\"))\n\nval minDate = mobilityData.select($\"Timestamp\").where($\"Timestamp\".isNotNull).orderBy(asc(\"Timestamp\")).first().mkString(\",\")\nval maxDate = mobilityData.select($\"Timestamp\").orderBy(desc(\"Timestamp\")).first().mkString(\",\")\nval datesString = f\"\"\"Min date: $minDate\nMax date: $maxDate\"\"\"\nprint(datesString)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Exception in thread Thread-146:\nTraceback (most recent call last):\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 196, in _check_jobs\n    self._send_msgs_for_fast_job(next_job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 290, in _send_msgs_for_fast_job\n    self._send_job_start(job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 375, in _send_job_start\n    stage = stage_dict[stageId]\nKeyError: 222\n\n"}, {"output_type": "stream", "name": "stdout", "text": "Min date: 2014-10-22 12:35:37.0\nMax date: 2014-11-06 13:28:25.0"}], "metadata": {"cell_status": {"execute_time": {"duration": 3284.164794921875, "end_time": 1619545733219.09}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 81, "cell_type": "code", "source": "/////////////////////////////////////////////\n/// Boundary coordinates for moblity data ///\n/////////////////////////////////////////////\n\nprintln(\"Longitude: \" + mobilityData.agg(min(\"Longitude\"), max(\"Longitude\")).head.toString() + \"\\nLatitude: \" + mobilityData.agg(min(\"Latitude\"), max(\"Latitude\")).head.toString())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Longitude: [-75.684,140.115]\nLatitude: [35.052,59.9254]"}], "metadata": {"cell_status": {"execute_time": {"duration": 2266.059814453125, "end_time": 1619542578481.927}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 33, "cell_type": "code", "source": "//////////////////\n/// Geohashing ///\n//////////////////\n\n// a user defined function to get geohash from long/lat point\nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}\n\nval precision = 30", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision: Int = 30"}], "metadata": {"cell_status": {"execute_time": {"duration": 1255.712890625, "end_time": 1619539765347.116}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 34, "cell_type": "code", "source": "////////////////////////////\n/// Geohash aerosol data ///\n////////////////////////////\n\nval geohashedAerosolData = (aerosolData\n                            .withColumn(\"index\", $\"point\" index  precision)\n                            .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedAerosolData = (geohashedAerosolData\n                                    .explode(\"geohashArray1\", \"geohash\")\n                                    { a: mutable.WrappedArray[String] => a })\nexplodedGeohashedAerosolData.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Latitude: double (nullable = true)\n |-- Longitude: double (nullable = true)\n |-- Value: double (nullable = true)\n |-- shortName: string (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Point: point (nullable = false)\n |-- index: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- curve: zordercurve (nullable = false)\n |    |    |-- relation: string (nullable = false)\n |-- geohashArray1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geohash: string (nullable = true)"}], "metadata": {"cell_status": {"execute_time": {"duration": 1252.55908203125, "end_time": 1619539766611.735}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 35, "cell_type": "code", "source": "/////////////////////////////\n/// Geohash mobility data ///\n/////////////////////////////\n\n\nval geohashedMobilityData = (mobilityData\n                         .withColumn(\"index\", $\"point\" index  precision)\n                         .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedMobilityData = (geohashedMobilityData\n                                 .explode(\"geohashArray1\", \"geohash\")\n                                 { a: mutable.WrappedArray[String] => a })\n\nexplodedGeohashedMobilityData.printSchema()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Code: string (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Value: double (nullable = true)\n |-- Latitude: double (nullable = true)\n |-- Longitude: double (nullable = true)\n |-- point: point (nullable = false)\n |-- index: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- curve: zordercurve (nullable = false)\n |    |    |-- relation: string (nullable = false)\n |-- geohashArray1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geohash: string (nullable = true)"}], "metadata": {"cell_status": {"execute_time": {"duration": 1255.1259765625, "end_time": 1619539767877.919}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 36, "cell_type": "code", "source": "/////////////////////////////////////////////////\n/// Import and Geohash the polygon of Bologna ///\n/////////////////////////////////////////////////\n\n// The final schema is unified as:\n// root\n//  |-- polygon: polygon (nullable = true)\n//  |-- index: array (nullable = false)\n//  |    |-- element: struct (containsNull = true)\n//  |    |    |-- curve: zordercurve (nullable = false)\n//  |    |    |-- relation: string (nullable = false)\n//  |-- Neighborhood: string (nullable = true)\n//  |-- Province: string (nullable = true)\n//  |-- Region: string (nullable = true)\n//  |-- geohashArray: array (nullable = true)\n//  |    |-- element: string (containsNull = true)\n//  |-- geohash: string (nullable = true)\n\nval rawBologna = (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/geojson/Bologna_quartieri.geojson\")\n                  .select($\"polygon\", $\"metadata\"(\"NOMEQUART\").as(\"Neighboorhood\"))\n                  )\nval bologna = (rawBologna\n               .withColumn(\"index\", $\"polygon\" index  precision)\n               .withColumn(\"Province\", lit(\"Bologna\"))\n               .withColumn(\"Region\", lit(\"Emilia-Romagna\"))\n               .select($\"polygon\", $\"index\", $\"Neighboorhood\", $\"Province\", $\"Region\")\n               .cache())\nval zorderIndexedBologna = (bologna\n                            .withColumn(\"index\", explode($\"index\"))\n                            .select(\"polygon\", \"index.curve\", \"index.relation\",\"Neighboorhood\", \"Province\", \"Region\")\n                          )\nval geohashedBologna = bologna.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedGeohashedBologna = geohashedBologna.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\nexplodedGeohashedBologna.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res142: Long = 426"}], "metadata": {"cell_status": {"execute_time": {"duration": 2268.1689453125, "end_time": 1619539770159.605}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 37, "cell_type": "code", "source": "explodedGeohashedBologna.select(\"*\").where(explodedGeohashedBologna(\"geohash\")===\"spzvpt\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+--------------+--------+--------------+--------------------+-------+\n|             polygon|               index| Neighboorhood|Province|        Region|        geohashArray|geohash|\n+--------------------+--------------------+--------------+--------+--------------+--------------------+-------+\n|magellan.Polygon@...|[[ZOrderCurve(11....|Borgo Panigale| Bologna|Emilia-Romagna|[spzvpt, spzvpw, ...| spzvpt|\n+--------------------+--------------------+--------------+--------+--------------+--------------------+-------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 1265.6689453125, "end_time": 1619539771437.733}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 38, "cell_type": "code", "source": "///////////////////////////////////////////////\n/// Import and Geohash the polygon of Italy ///\n///////////////////////////////////////////////\n\n// The final schema is unified as:\n// root\n//  |-- polygon: polygon (nullable = true)\n//  |-- index: array (nullable = false)\n//  |    |-- element: struct (containsNull = true)\n//  |    |    |-- curve: zordercurve (nullable = false)\n//  |    |    |-- relation: string (nullable = false)\n//  |-- Neighborhood: string (nullable = true)\n//  |-- Province: string (nullable = true)\n//  |-- Region: string (nullable = true)\n//  |-- geohashArray: array (nullable = true)\n//  |    |-- element: string (containsNull = true)\n//  |-- geohash: string (nullable = true)\n\n\nval rawItaly = (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/geojson/Italy_quartieri.geojson\")\n                  .select($\"polygon\",\n                          $\"metadata\"(\"name\").as(\"Neighborhood\"),\n                          $\"metadata\"(\"prov_name\").as(\"Province\"),\n                          $\"metadata\"(\"reg_name\").as(\"Region\"))\n                  )\nval italy = (rawItaly\n               .withColumn(\"index\", $\"polygon\" index  precision)\n               .select($\"polygon\", $\"index\", $\"Neighborhood\", $\"Province\", $\"Region\")\n               .cache())\nval zorderIndexedItaly = (italy\n                            .withColumn(\"index\", explode($\"index\"))\n                            .select(\"polygon\", \"index.curve\", \"index.relation\", \"Neighborhood\", \"Province\", \"Region\")\n                          )\nval geohashedItaly = italy.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedGeohashedItaly = geohashedItaly.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\nexplodedGeohashedItaly.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res163: Long = 752793"}], "metadata": {"cell_status": {"execute_time": {"duration": 3275.281005859375, "end_time": 1619539774726.537}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 39, "cell_type": "code", "source": "explodedGeohashedItaly.select(\"Province\").where($\"Province\" === \"Bologna\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+\n|Province|\n+--------+\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n| Bologna|\n+--------+\nonly showing top 20 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 746.4560546875, "end_time": 1619539775483.554}}, "collapsed": false}}, {"execution_count": 40, "cell_type": "code", "source": "val explodedGeohashedItalyNoBologna = explodedGeohashedItaly.filter(col(\"Neighborhood\") =!= \"Bologna\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "explodedGeohashedItalyNoBologna: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 5 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 753.087890625, "end_time": 1619539776247.941}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 41, "cell_type": "code", "source": "explodedGeohashedItalyNoBologna.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- polygon: polygon (nullable = true)\n |-- index: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- curve: zordercurve (nullable = false)\n |    |    |-- relation: string (nullable = false)\n |-- Neighborhood: string (nullable = true)\n |-- Province: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- geohashArray: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geohash: string (nullable = true)"}], "metadata": {"cell_status": {"execute_time": {"duration": 744.31591796875, "end_time": 1619539777002.917}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 42, "cell_type": "code", "source": "explodedGeohashedBologna.printSchema()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- polygon: polygon (nullable = true)\n |-- index: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- curve: zordercurve (nullable = false)\n |    |    |-- relation: string (nullable = false)\n |-- Neighboorhood: string (nullable = true)\n |-- Province: string (nullable = false)\n |-- Region: string (nullable = false)\n |-- geohashArray: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- geohash: string (nullable = true)"}], "metadata": {"cell_status": {"execute_time": {"duration": 743.88623046875, "end_time": 1619539777757.668}}, "collapsed": false}}, {"execution_count": 43, "cell_type": "code", "source": "val allCities = explodedGeohashedItalyNoBologna.union(explodedGeohashedBologna)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "allCities: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 5 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 748.073974609375, "end_time": 1619539778517.285}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 54, "cell_type": "code", "source": "val rawTripsJoinedBSO = (explodedGeohashedMobilityData\n                         .join(\n                             allCities, \n                             explodedGeohashedMobilityData(\"geohash\") === allCities(\"geohash\"))\n                         .select(\"point\", \"Neighborhood\", \"Province\", \"Region\", \"Timestamp\", \"Code\", \"Value\", \"Latitude\", \"Longitude\")\n                         .where($\"point\" within $\"polygon\").cache())\n// val min_lat = rawTripsJoinedBSO.select($\"Latitude\").distinct.orderBy(asc(\"Latitude\")).first().mkString(\",\").toDouble\n// val max_lat = rawTripsJoinedBSO.select($\"Latitude\").distinct.orderBy(desc(\"Latitude\")).first().mkString(\",\").toDouble\n// val min_lon = rawTripsJoinedBSO.select($\"Longitude\").distinct.orderBy(asc(\"Longitude\")).first().mkString(\",\").toDouble\n// val max_lon = rawTripsJoinedBSO.select($\"Longitude\").distinct.orderBy(desc(\"Longitude\")).first().mkString(\",\").toDouble\n// val coordinates_string = f\"\"\"The coordinate extremes are:\n// Longitude: ($min_lon, $max_lon)\n// Latitude: ($min_lat, $max_lat)\"\"\"\n", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Exception in thread Thread-97:\nTraceback (most recent call last):\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 192, in _check_jobs\n    self._send_job_start(next_job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 375, in _send_job_start\n    stage = stage_dict[stageId]\nKeyError: 165\n\n"}, {"output_type": "stream", "name": "stdout", "text": "Min date: 2014-10-22 12:35:37.0\nMax date: 2014-11-06 13:28:25.0"}], "metadata": {"cell_status": {"execute_time": {"duration": 3284.35302734375, "end_time": 1619541918402.609}}, "collapsed": false}}, {"execution_count": 82, "cell_type": "code", "source": "val minDate = rawTripsJoinedBSO.select(\"Timestamp\").where($\"Timestamp\".isNotNull).orderBy(asc(\"Timestamp\")).first().mkString(\",\")\nval maxDate = rawTripsJoinedBSO.select(\"Timestamp\").orderBy(desc(\"Timestamp\")).first().mkString(\",\")\nval datesString = f\"\"\"Min date: $minDate\nMax date: $maxDate\"\"\"\nprint(datesString)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Exception in thread Thread-145:\nTraceback (most recent call last):\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 196, in _check_jobs\n    self._send_msgs_for_fast_job(next_job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 290, in _send_msgs_for_fast_job\n    self._send_job_start(job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 375, in _send_job_start\n    stage = stage_dict[stageId]\nKeyError: 219\n\n"}, {"output_type": "stream", "name": "stdout", "text": "Min date: 2014-10-22 12:35:37.0\nMax date: 2014-11-06 13:28:25.0"}], "metadata": {"cell_status": {"execute_time": {"duration": 2265.928955078125, "end_time": 1619542612723.883}}, "collapsed": false}}, {"execution_count": 45, "cell_type": "code", "source": "rawTripsJoinedBSO.select($\"Longitude\").distinct.orderBy(desc(\"Longitude\")).first().mkString(\",\").toDouble", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res168: Double = 18.2276"}], "metadata": {"cell_status": {"execute_time": {"duration": 2317.39404296875, "end_time": 1619539800285.554}}, "collapsed": false}}, {"execution_count": 48, "cell_type": "code", "source": "///////////////////////////////////////////\n/// Join CAMS data with the whole Italy ///\n///////////////////////////////////////////\nval rawCAMSJoinedBSO = (explodedGeohashedAerosolData\n                         .join(\n                             allCities, \n                             explodedGeohashedAerosolData(\"geohash\") === allCities(\"geohash\"))\n                         .select(\"point\", \"Neighborhood\", \"Province\", \"Region\", \"Timestamp\", \"Value\", \"Latitude\", \"Longitude\")\n                         .where($\"point\" within $\"polygon\"))\n\nval min_lat = rawCAMSJoinedBSO.select($\"Latitude\").distinct.orderBy(asc(\"Latitude\")).first().mkString(\",\").toDouble\nval max_lat = rawCAMSJoinedBSO.select($\"Latitude\").distinct.orderBy(desc(\"Latitude\")).first().mkString(\",\").toDouble\nval min_lon = rawCAMSJoinedBSO.select($\"Longitude\").distinct.orderBy(asc(\"Longitude\")).first().mkString(\",\").toDouble\nval max_lon = rawCAMSJoinedBSO.select($\"Longitude\").distinct.orderBy(desc(\"Longitude\")).first().mkString(\",\").toDouble\nval coordinates_string = f\"\"\"The coordinate extremes are:\nLongitude: ($min_lon, $max_lon)\nLatitude: ($min_lat, $max_lat)\"\"\"\nprint(coordinates_string)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The coordinate extremes are:\nLongitude: (8.207, 17.958)\nLatitude: (37.979, 46.229)"}], "metadata": {"cell_status": {"execute_time": {"duration": 13333.09814453125, "end_time": 1619541534125.527}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}