{"nbformat_minor": 2, "cells": [{"execution_count": 3, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', u'spark.dynamicAllocation.enabled': False, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 18.18994140625, "end_time": 1619192208652.879}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "/**\n * @Description: a spatial join based on Filter-refine approach for NYC taxicab data\n * @author: Isam Al Jawarneh\n * @date: 02/02/2019\n *last update: 14/04/2021\n */", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1619184318710_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sspark.y43epnzqhm4uzldxnrp41qfghf.fx.internal.cloudapp.net:8088/proxy/application_1619184318710_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-sspark.y43epnzqhm4uzldxnrp41qfghf.fx.internal.cloudapp.net:30060/node/containerlogs/container_1619184318710_0004_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 777.796142578125, "end_time": 1619192262790.098}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "sc.version", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res3: String = 2.2.0.2.6.3.84-1"}], "metadata": {"cell_status": {"execute_time": {"duration": 772.902099609375, "end_time": 1619192263574.232}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.DataFrame"}], "metadata": {"cell_status": {"execute_time": {"duration": 9355.256103515625, "end_time": 1619192272943.403}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 7, "cell_type": "code", "source": "/////////////////////////////\n/// Definition of schemas ///\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 764.304931640625, "end_time": 1619192286057.202}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 9, "cell_type": "code", "source": "val aerosolDataSchema = StructType(Array(\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"dataDate\", StringType, false),\n    StructField(\"time\", StringType, false),\n    StructField(\"shortName\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "aerosolDataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Latitude,DoubleType,false), StructField(Longitude,DoubleType,false), StructField(Value,DoubleType,false), StructField(dataDate,StringType,false), StructField(time,StringType,false), StructField(shortName,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 1275.72802734375, "end_time": 1619192301160.905}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "/////////////////////////////\n///// Import Dataframes /////\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.864013671875, "end_time": 1619192304413.104}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 17, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval aerosolData = (spark.read.format(\"csv\")\n                        .option(\"header\", \"true\")\n                        .schema(aerosolDataSchema)\n                        .csv(\"wasbs://sspark-2021-04-23t13-18-44-008z@ssparkhdistorage.blob.core.windows.net/cams_data/*\")\n                        .withColumn(\"timestamp\", to_timestamp(concat($\"dataDate\", lit(\" \"), $\"time\"), \"yyyyMMdd HHmm\"))\n                        .withColumn(\"Point\", point($\"Longitude\",$\"Latitude\"))\n                        .drop(\"Longitude\", \"Latitude\", \"dataDate\", \"time\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "aerosolData: org.apache.spark.sql.DataFrame = [Value: double, shortName: string ... 2 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 1278.2490234375, "end_time": 1619193972312.419}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 19, "cell_type": "code", "source": "// aerosolData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Value: double (nullable = true)\n |-- shortName: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- Point: point (nullable = false)"}], "metadata": {"cell_status": {"execute_time": {"duration": 764.827880859375, "end_time": 1619194000358.695}}, "collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "//////////////////\n/// Geohashing ///\n//////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.744873046875, "end_time": 1619192315638.589}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 13, "cell_type": "code", "source": "// a user defined function to get geohash from long/lat point \nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@3cec181,true))))"}], "metadata": {"cell_status": {"execute_time": {"duration": 2283.848876953125, "end_time": 1619192317934.94}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 14, "cell_type": "code", "source": "val precision = 30", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision: Int = 30"}], "metadata": {"cell_status": {"execute_time": {"duration": 767.27294921875, "end_time": 1619192318714.723}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 15, "cell_type": "code", "source": "//getting plain data from CSV file (file with point Data Structure) and use UDF to get geohashes\nval geohashedAerosolData = (aerosolData\n                         .withColumn(\"index\", $\"point\" index  precision)\n                         .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedAerosolData = (geohashedAerosolData\n                                 .explode(\"geohashArray1\", \"geohash\")\n                                 { a: mutable.WrappedArray[String] => a })", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedGeohashedAerosolData: org.apache.spark.sql.DataFrame = [Value: double, dataDate: date ... 6 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 2291.704833984375, "end_time": 1619192321019.128}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 16, "cell_type": "code", "source": "explodedGeohashedAerosolData.show(2,false)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------------+----------+-------------------+---------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|Value          |dataDate  |time               |shortName|Point             |index                                                                                                                                                 |geohashArray1|geohash|\n+---------------+----------+-------------------+---------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|8.0796069835E-9|2020-01-01|1970-01-01 12:00:00|pm10     |Point(43.0, 11.5) |[[ZOrderCurve(42.989501953125, 11.4971923828125, 43.00048828125, 11.502685546875, 30, -4348063485599416320, 110000111010100010010011010011),Contains]]|[sfn96m]     |sfn96m |\n|4.8530781616E-9|2020-01-01|1970-01-01 12:00:00|pm10     |Point(43.75, 11.5)|[[ZOrderCurve(43.74755859375, 11.4971923828125, 43.758544921875, 11.502685546875, 30, -4347640620299321344, 110000111010101000010011111001),Contains]]|[sfp17t]     |sfp17t |\n+---------------+----------+-------------------+---------+------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 9373.697021484375, "end_time": 1619192334567.164}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// To be substituted with the city we choose for the presentation\n\n// val rawCity= (spark.read.format(\"magellan\")\n//                   .option(\"type\", \"geojson\")\n//                  // .load(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/bologna/\")\n//                   .select($\"polygon\", $\"metadata\"(\"NOME_COM\").as(\"City_Name\")).cache()\n//                   )\n// val city = (rawCity\n//                .withColumn(\"index\", $\"polygon\" index  precision)\n//                .select($\"polygon\", $\"index\", $\"City_Name\")\n//                .cache())\n// val zorderIndexedCity = (city\n//                             .withColumn(\"index\", explode($\"index\"))\n//                             .select(\"polygon\", \"index.curve\", \"index.relation\",\"City_Name\")\n//                           )\n// val geohashedCity = city.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n// val explodedGeohashedCity = geohashedCity.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n// explodedGeohashedCity.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5308.705810546875, "end_time": 1618778840640.972}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//joining geohashed trips with exploded geohashed neighborhood using filter-and-refine approach (.where($\"point\" within $\"polygon\") is refine --> using the brute force method ray casting for edge cases or false positives)\nval aerosolDataInCity = (explodedGeohashedCity\n                         .join(explodedGeohashedAerosolData,\n                               explodedGeohashedCity(\"geohash\") === explodedGeohashedAerosolData(\"geohash\"))\n                         .where($\"point\" within $\"polygon\")\n                        )\naerosolDataInCity.show(3)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2277.087158203125, "end_time": 1618778842930.201}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "aerosolDataInCity.columns", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 757.93896484375, "end_time": 1618778843699.442}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// val airDataWithParameters = (airData\n//                              .join(\n//                                  parametersRegistry,\n//                                  airData(\"Parameter_Id\") === parametersRegistry(\"Parameter_Id\")))\n// airDataWithParameters.show(2)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2271.432861328125, "end_time": 1618778845983.902}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// val finalTable = (airDataWithParameters\n//                   .join(stationsInEmiliaRomagna,\n//                         airDataWithParameters(\"Station_Code\") === stationsInEmiliaRomagna(\"Station_Code\")))\n// finalTable.show(3)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 9318.9619140625, "end_time": 1618778855315.485}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// finalTable.printSchema()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 753.929931640625, "end_time": 1618778856082.027}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}