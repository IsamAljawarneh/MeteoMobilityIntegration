{"nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": true,\n        \"spark.shuffle.service.enabled\": true,\n        \"spark.rdd.compress\": true,\n        \"spark.default.parallelism\": 2010,\n        \"spark.sql.shuffle.partitions\": 2010\n    }\n}\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 28.52294921875, "end_time": 1619803206252.618}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 9328.66796875, "end_time": 1619803249854.944}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val containerStorageName = \"denis-2021-04-27t09-50-33-745z\"\nval storageAccountName = \"denishdistorage\"", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.35205078125, "end_time": 1619803250629.511}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////////////\n/// Definition of schemas ///\n/////////////////////////////\n\nval aerosolDataSchema = StructType(Array(\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"dataDate\", StringType, false),\n    StructField(\"time\", StringType, false),\n    StructField(\"shortName\", StringType, false)))\n\nval mobilityDataSchema = StructType(Array(\n    StructField(\"Code\", StringType, false),\n    StructField(\"Timestamp\", StringType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Status\", StringType, false),\n    StructField(\"Other_Value\", DoubleType, false),\n    StructField(\"Other_Date\", StringType, false)))\n\nval pm10DataSchema = StructType(Array(\n    StructField(\"lat\", DoubleType, false),\n    StructField(\"lon\", DoubleType, false),\n    StructField(\"pm10_value\", DoubleType, false)))\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2285.19091796875, "end_time": 1619803252928.3}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "///////////////////////////////\n/// Mobility data filenames ///\n///////////////////////////////\n\nval mob20k = \"db20mila.csv\"\nval mob100k = \"db100mila.csv\"\nval mob250k = \"db250mila.csv\"\nval mob500k = \"db500mila.csv\"", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1266.753173828125, "end_time": 1619803254207.285}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////////////\n///// Import Dataframes /////\n/////////////////////////////\n\n//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval aerosolData = (spark.read.format(\"csv\")\n                   .option(\"header\", \"true\")\n                   .schema(aerosolDataSchema)\n                   .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/cams_air_data/cams_data_italy_right_dates.csv\")\n                   .withColumn(\"Timestamp\", to_timestamp(concat($\"dataDate\", lit(\" \"), $\"time\"), \"yyyyMMdd HHmm\"))\n                   .withColumn(\"Point\", point($\"Longitude\",$\"Latitude\"))\n                   .drop(\"dataDate\", \"time\")\n                   .repartition(100))\n\nval mobilityData = (spark.read.format(\"csv\")\n                    .option(\"header\", \"true\")\n                    .option(\"delimiter\", \";\")\n                    .schema(mobilityDataSchema)\n                    .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + f\".blob.core.windows.net/data/mobility_data/$mob500k\")\n                    .withColumn(\"Timestamp\", to_timestamp($\"Timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n                    .withColumn(\"Point\", point($\"Longitude\",$\"Latitude\"))\n                    .drop(\"Status\", \"Other_Value\", \"Other_Date\")\n                    .repartition(500))\n\nval pm10Data = (spark.read.format(\"csv\")\n                .option(\"header\", \"true\")\n                .schema(pm10DataSchema)\n                .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/copernicus_air_data/pm10.csv\")\n                .withColumn(\"Point\", point($\"lon\",$\"lat\"))\n                .repartition(200))\n\n// val minDate = mobilityData.select($\"Timestamp\").where($\"Timestamp\".isNotNull).orderBy(asc(\"Timestamp\")).first().mkString(\",\")\n// val maxDate = mobilityData.select($\"Timestamp\").orderBy(desc(\"Timestamp\")).first().mkString(\",\")\n// val datesString = f\"\"\"Min date: $minDate\n// Max date: $maxDate\"\"\"\n// print(datesString)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7332.14697265625, "end_time": 1619803261552.547}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// mobilityData.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.533935546875, "end_time": 1619803262324.465}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////////////////////////////\n/// Boundary coordinates for moblity data ///\n/////////////////////////////////////////////\n\n// println(\"Longitude: \" + mobilityData.agg(min(\"Longitude\"), max(\"Longitude\")).head.toString() + \"\\nLatitude: \" + mobilityData.agg(min(\"Latitude\"), max(\"Latitude\")).head.toString())", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 769.615966796875, "end_time": 1619803263107.365}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//////////////////\n/// Geohashing ///\n//////////////////\n\n// a user defined function to get geohash from long/lat point\nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}\n\nval precision: Int = 30", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2277.761962890625, "end_time": 1619803265398.211}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "////////////////////////////\n/// Geohash aerosol data ///\n////////////////////////////\n\nval geohashedAerosolData = (aerosolData\n                            .withColumn(\"Index\", $\"Point\" index  precision)\n                            .withColumn)(\"GeohashArray1\", geohashUDF($\"Index.curve\"))\nval explodedGeohashedAerosolData = (geohashedAerosolData\n                                    .explode(\"GeohashArray1\", \"Geohash\")\n                                    { a: mutable.WrappedArray[String] => a })\n\nvar explodedGeohashedAerosolDataLight = explodedGeohashedAerosolData.select(\"Value\", \"shortName\", \"Point\", \"Geohash\")\n\n// Renaming of DF columns with prefix for identifying columns in later join\nfor (column <- explodedGeohashedAerosolDataLight.columns) {\n    explodedGeohashedAerosolDataLight = (explodedGeohashedAerosolDataLight\n                                    .withColumn(\"aerosol_\" + column, explodedGeohashedAerosolDataLight(column))\n                                    .drop(column))\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2282.108154296875, "end_time": 1619803267692.255}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////////////\n/// Geohash mobility data ///\n/////////////////////////////\n\n\nval geohashedMobilityData = (mobilityData\n                         .withColumn(\"Index\", $\"Point\" index  precision)\n                         .withColumn(\"GeohashArray1\", geohashUDF($\"Index.curve\")))\nval explodedGeohashedMobilityData = (geohashedMobilityData\n                                 .explode(\"GeohashArray1\", \"Geohash\")\n                                 { a: mutable.WrappedArray[String] => a })\nvar explodedGeohashedMobilityDataLight = (explodedGeohashedMobilityData\n                                          .select(\"Code\", \"Value\", \"Point\", \"Geohash\"))\n\n// Renaming of DF columns with prefix for identifying columns in later join\nfor (column <- explodedGeohashedMobilityDataLight.columns) {\n    explodedGeohashedMobilityDataLight = (explodedGeohashedMobilityDataLight\n                                    .withColumn(\"mobility_\" + column, explodedGeohashedMobilityDataLight(column))\n                                    .drop(column))\n}\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2288.65478515625, "end_time": 1619803269994.554}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////////\n/// Geohash pm10 data ///\n/////////////////////////\n\nval geohashedPm10Data = (pm10Data\n                         .withColumn(\"Index\", $\"Point\" index  precision)\n                         .withColumn(\"GeohashArray1\", geohashUDF($\"Index.curve\")))\nval explodedGeohashedPm10Data = (geohashedPm10Data\n                                 .explode(\"GeohashArray1\", \"Geohash\")\n                                 { a: mutable.WrappedArray[String] => a })\nvar explodedGeohashedPm10DataLight = (explodedGeohashedPm10Data\n                                      .select($\"lat\".as(\"Latitude\"), $\"lon\".as(\"Longitude\"),\n                                              $\"pm10_value\".as(\"Value\"), $\"Point\", $\"Geohash\"))\n\n// Renaming of DF columns with prefix for identifying columns in later join\nfor (column <- explodedGeohashedPm10DataLight.columns) {\n    explodedGeohashedPm10DataLight = (explodedGeohashedPm10DataLight\n                                 .withColumn(\"pm10_\" + column, explodedGeohashedPm10DataLight(column))\n                                 .drop(column))\n}\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2289.903076171875, "end_time": 1619803272297.832}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////////////////////////////////\n/// Import and Geohash the polygon of Bologna ///\n/////////////////////////////////////////////////\n\n// The final schema is unified as:\n// root\n//  |-- polygon: polygon (nullable = true)\n//  |-- index: array (nullable = false)\n//  |    |-- element: struct (containsNull = true)\n//  |    |    |-- curve: zordercurve (nullable = false)\n//  |    |    |-- relation: string (nullable = false)\n//  |-- Neighborhood: string (nullable = true)\n//  |-- Province: string (nullable = true)\n//  |-- Region: string (nullable = true)\n//  |-- geohashArray: array (nullable = true)\n//  |    |-- element: string (containsNull = true)\n//  |-- geohash: string (nullable = true)\n\nval rawBologna = (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/geojson/Bologna_quartieri.geojson\")\n                  .select($\"polygon\", $\"metadata\"(\"NOMEQUART\").as(\"Neighboorhood\"))\n                  )\n\nval bologna = (rawBologna\n               .withColumn(\"index\", $\"polygon\" index  precision)\n               .withColumn(\"Province\", lit(\"Bologna\"))\n               .withColumn(\"Region\", lit(\"Emilia-Romagna\"))\n               .select($\"polygon\", $\"index\", $\"Neighboorhood\", $\"Province\", $\"Region\"))\n\nval zorderIndexedBologna = (bologna\n                            .withColumn(\"index\", explode($\"index\"))\n                            .select(\"polygon\", \"index.curve\", \"index.relation\",\"Neighboorhood\", \"Province\", \"Region\")\n                          )\nval geohashedBologna = bologna.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedGeohashedBologna = geohashedBologna.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2283.04296875, "end_time": 1619803274598.179}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "///////////////////////////////////////////////\n/// Import and Geohash the polygon of Italy ///\n///////////////////////////////////////////////\n\n// The final schema is unified as:\n// root\n//  |-- polygon: polygon (nullable = true)\n//  |-- index: array (nullable = false)\n//  |    |-- element: struct (containsNull = true)\n//  |    |    |-- curve: zordercurve (nullable = false)\n//  |    |    |-- relation: string (nullable = false)\n//  |-- Neighborhood: string (nullable = true)\n//  |-- Province: string (nullable = true)\n//  |-- Region: string (nullable = true)\n//  |-- geohashArray: array (nullable = true)\n//  |    |-- element: string (containsNull = true)\n//  |-- geohash: string (nullable = true)\n\n\nval rawItaly = (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/geojson/Italy_quartieri.geojson\")\n                  .select($\"Polygon\",\n                          $\"metadata\"(\"name\").as(\"Neighborhood\"),\n                          $\"metadata\"(\"prov_name\").as(\"Province\"),\n                          $\"metadata\"(\"reg_name\").as(\"Region\"))\n                  )\nval italy = (rawItaly\n               .withColumn(\"Index\", $\"polygon\" index  precision)\n               .select($\"Polygon\", $\"Index\", $\"Neighborhood\", $\"Province\", $\"Region\"))\n\nval zorderIndexedItaly = (italy\n                            .withColumn(\"Index\", explode($\"Index\"))\n                            .select(\"Polygon\", \"Index.curve\", \"Index.relation\", \"Neighborhood\", \"Province\", \"Region\")\n                          )\nval geohashedItaly = italy.withColumn(\"GeohashArray\", geohashUDF($\"Index.curve\"))\nval explodedGeohashedItaly = geohashedItaly.explode(\"GeohashArray\", \"Geohash\") { a: mutable.WrappedArray[String] => a }", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2348.19287109375, "end_time": 1619803276960.831}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val explodedGeohashedItalyNoBologna = explodedGeohashedItaly.filter(col(\"Neighborhood\") =!= \"Bologna\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 764.3408203125, "end_time": 1619803277738.134}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "var allCities = explodedGeohashedItalyNoBologna.union(explodedGeohashedBologna).repartition(1000)\n\nfor (column <- allCities.columns) {\n    allCities = (allCities\n                 .withColumn(\"cities_\" + column, allCities(column))\n                 .drop(column))\n}\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1267.218994140625, "end_time": 1619803279019.075}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val meteoMobilityIntegratedDataframe = (allCities\n                                        .join(explodedGeohashedMobilityDataLight,\n                                               $\"mobility_Geohash\" === $\"cities_Geohash\")\n                                        .join(\n                                            explodedGeohashedPm10DataLight, \n                                            $\"pm10_Geohash\" === $\"cities_Geohash\")\n                                        .where($\"pm10_Point\" within $\"cities_Polygon\")\n                                        .select($\"cities_Polygon\".as(\"Polygon\"), $\"pm10_Longitude\".as(\"Longitude\"),\n                                                $\"pm10_Latitude\".as(\"Latitude\"), $\"pm10_Point\".as(\"Point\"),\n                                                $\"cities_Neighborhood\".as(\"Neighborhood\"),\n                                                $\"cities_Province\".as(\"Province\"), $\"cities_Region\".as(\"Region\"),\n                                                $\"cities_Geohash\".as(\"Geohash\"), $\"mobility_Code\".as(\"Trip_Code\"),\n                                                $\"mobility_Value\".as(\"Trip_Value\"), $\"pm10_Value\".as(\"PM10_Value\"))\n                                        .cache()\n                                       )\n\n// Saving the output of SparkSession.time() output is unreliable, as the time elapsed is only printed to stdout,\n// meanwhile what is returned is the output of the function, as shown in the SparkSession source code:\n// \n/**\n   * Executes some code block and prints to stdout the time taken to execute the block. This is\n   * available in Scala only and is used primarily for interactive testing and debugging.\n   *\n   * @since 2.1.0\n   */\n// def time[T](f: => T): T = {\n//     val start = System.nanoTime()\n//     val ret = f\n//     val end = System.nanoTime()\n//     // scalastyle:off println\n//     println(s\"Time taken: ${NANOSECONDS.toMillis(end - start)} ms\")\n//     // scalastyle:on println\n//     ret\n//   }\n\nvar linesWritten = spark.time(meteoMobilityIntegratedDataframe.count())\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 83881.51196289062, "end_time": 1619803362915.352}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val timeTaken = 81305d\nval baseThroughput = linesWritten/timeTaken*1000d\nprintln(\"Lines written: \"+ linesWritten + \" lines\\nTime elapsed: \" + timeTaken + \n        \" ms\\nThroughput: \" + baseThroughput + \" lines/s\")\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1296.8759765625, "end_time": 1619803492320.639}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "// ///////////////////////////\n// /// Top K spatial query ///\n// ///////////////////////////\n\n\nspark.time(meteoMobilityIntegratedDataframe\n .select(\"*\")\n .groupBy(col(\"Neighborhood\"))\n .agg(count(\"*\").as(\"count\"))\n .sort(desc(\"count\"))\n .show(1))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 757.19091796875, "end_time": 1619803364964.509}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/////////////////////\n/// Average query ///\n/////////////////////\n\nspark.time(meteoMobilityIntegratedDataframe.groupBy($\"Neighborhood\").agg(avg($\"PM10_Value\")).show(1))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 755.988037109375, "end_time": 1619803365735.307}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": " ", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}