{"nbformat_minor": 2, "cells": [{"execution_count": 101, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1619430760498_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:8088/proxy/application_1619430760498_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:30060/node/containerlogs/container_1619430760498_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', u'spark.dynamicAllocation.enabled': False, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1619430760498_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:8088/proxy/application_1619430760498_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:30060/node/containerlogs/container_1619430760498_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 768.85302734375, "end_time": 1619445391426.773}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 102, "cell_type": "code", "source": "/**\n * @Description: a spatial join based on Filter-refine approach for NYC taxicab data\n * @author: Isam Al Jawarneh\n * @date: 02/02/2019\n *last update: 14/04/2021\n */", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 762.20703125, "end_time": 1619445392202.678}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 103, "cell_type": "code", "source": "sc.version", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res3: String = 2.2.0.2.6.3.84-1"}], "metadata": {"cell_status": {"execute_time": {"duration": 758.426025390625, "end_time": 1619445392974.862}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 104, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.DataFrame"}], "metadata": {"cell_status": {"execute_time": {"duration": 7324.972900390625, "end_time": 1619445400313.821}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 105, "cell_type": "code", "source": "val containerStorageName = \"meteomobilitysparkdenis-2021-04-24t15-54-44-656z\"\nval storageAccountName = \"meteomobilityhdistorage\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "storageAccountName: String = meteomobilityhdistorage"}], "metadata": {"cell_status": {"execute_time": {"duration": 757.258056640625, "end_time": 1619445401083.156}}, "collapsed": false}}, {"execution_count": 106, "cell_type": "code", "source": "/////////////////////////////\n/// Definition of schemas ///\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 759.349853515625, "end_time": 1619445401854.988}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 107, "cell_type": "code", "source": "val aerosolDataSchema = StructType(Array(\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"dataDate\", StringType, false),\n    StructField(\"time\", StringType, false),\n    StructField(\"shortName\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "aerosolDataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Latitude,DoubleType,false), StructField(Longitude,DoubleType,false), StructField(Value,DoubleType,false), StructField(dataDate,StringType,false), StructField(time,StringType,false), StructField(shortName,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 1276.26904296875, "end_time": 1619445403142.301}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 127, "cell_type": "code", "source": "val mobilityDataSchema = StructType(Array(\n    StructField(\"Code\", StringType, false),\n    StructField(\"Timestamp\", StringType, false),\n    StructField(\"Value\", DoubleType, false),\n    StructField(\"Latitude\", DoubleType, false),\n    StructField(\"Longitude\", DoubleType, false),\n    StructField(\"Status\", StringType, false),\n    StructField(\"Other_Value\", DoubleType, false),\n    StructField(\"Other_Date\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "mobilityDataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Code,StringType,false), StructField(Timestamp,StringType,false), StructField(Value,DoubleType,false), StructField(Latitude,DoubleType,false), StructField(Longitude,DoubleType,false), StructField(Status,StringType,false), StructField(Other_Value,DoubleType,false), StructField(Other_Date,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 761.656005859375, "end_time": 1619445799605.33}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 109, "cell_type": "code", "source": "/////////////////////////////\n///// Import Dataframes /////\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 760.742919921875, "end_time": 1619445404686.142}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 110, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval aerosolData = (spark.read.format(\"csv\")\n                        .option(\"header\", \"true\")\n                        .schema(aerosolDataSchema)\n                        .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/cams_air_data/*\")\n                        .withColumn(\"timestamp\", to_timestamp(concat($\"dataDate\", lit(\" \"), $\"time\"), \"yyyyMMdd HHmm\"))\n                        .withColumn(\"Point\", point($\"Longitude\",$\"Latitude\"))\n                        .drop(\"Longitude\", \"Latitude\", \"dataDate\", \"time\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "aerosolData: org.apache.spark.sql.DataFrame = [Value: double, shortName: string ... 2 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 5353.652099609375, "end_time": 1619445410055.645}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 128, "cell_type": "code", "source": "val mobilityData = (spark.read.format(\"csv\")\n                    .option(\"header\", \"true\")\n                    .option(\"delimiter\", \";\")\n                    .schema(mobilityDataSchema)\n                    .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/mobility_data/db250mila.csv\")\n                    .withColumn(\"Timestamp\", to_timestamp($\"Timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))\n                    .withColumn(\"point\", point($\"Longitude\",$\"Latitude\"))\n                    .drop(\"Status\", \"Other_Value\", \"Other_Date\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "mobilityData: org.apache.spark.sql.DataFrame = [Code: string, Timestamp: timestamp ... 4 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 755.968017578125, "end_time": 1619445804254.843}}, "collapsed": false}}, {"execution_count": 129, "cell_type": "code", "source": "val Row(minLon: Double, maxLon: Double) = mobilityData.agg(min(\"Longitude\"), max(\"Longitude\")).head", "outputs": [{"output_type": "stream", "name": "stdout", "text": "minLon: Double = -75.684\nmaxLon: Double = 140.115"}], "metadata": {"cell_status": {"execute_time": {"duration": 1274.2529296875, "end_time": 1619445808313.577}}, "collapsed": false}}, {"execution_count": 130, "cell_type": "code", "source": "val Row(minLat: Double, maxLat: Double) = mobilityData.agg(min(\"Latitude\"), max(\"Latitude\")).head", "outputs": [{"output_type": "stream", "name": "stdout", "text": "minLat: Double = 35.052\nmaxLat: Double = 59.9254"}], "metadata": {"cell_status": {"execute_time": {"duration": 1262.803955078125, "end_time": 1619445809593.715}}, "collapsed": false}}, {"execution_count": 131, "cell_type": "code", "source": "mobilityData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+-------------------+-----+--------+---------+--------------------+\n|    Code|          Timestamp|Value|Latitude|Longitude|               point|\n+--------+-------------------+-----+--------+---------+--------------------+\n|20091560|2014-10-22 12:35:37| 33.0|   43.58|  13.5056|Point(13.5056, 43...|\n|20091561|2014-10-22 12:35:37| 42.0|   43.58|  13.5056|Point(13.5056, 43...|\n|20091562|2014-10-22 12:35:37| 36.0|   43.58|  13.5056|Point(13.5056, 43...|\n|20091563|2014-10-22 12:35:37| 37.5|   43.58|  13.5056|Point(13.5056, 43...|\n|20091564|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091565|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091566|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091567|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091568|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091569|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091570|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091571|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091572|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091573|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091574|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091575|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091576|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091577|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091578|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n|20091579|2014-10-22 12:35:37| 33.0|   43.58|  13.5057|Point(13.5057, 43...|\n+--------+-------------------+-----+--------+---------+--------------------+\nonly showing top 20 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 751.657958984375, "end_time": 1619445817963.118}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 115, "cell_type": "code", "source": "//////////////////\n/// Geohashing ///\n//////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 752.3779296875, "end_time": 1619445422632.741}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 132, "cell_type": "code", "source": "// a user defined function to get geohash from long/lat point \nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@61007700,true))))"}], "metadata": {"cell_status": {"execute_time": {"duration": 779.4140625, "end_time": 1619445826689.836}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 133, "cell_type": "code", "source": "val precision = 30", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision: Int = 30"}], "metadata": {"cell_status": {"execute_time": {"duration": 759.816162109375, "end_time": 1619445827462.461}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 134, "cell_type": "code", "source": "//getting plain data from CSV file (file with point Data Structure) and use UDF to get geohashes\nval geohashedAerosolData = (aerosolData\n                         .withColumn(\"index\", $\"point\" index  precision)\n                         .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedAerosolData = (geohashedAerosolData\n                                 .explode(\"geohashArray1\", \"geohash\")\n                                 { a: mutable.WrappedArray[String] => a })", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedGeohashedAerosolData: org.apache.spark.sql.DataFrame = [Value: double, shortName: string ... 5 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 754.60693359375, "end_time": 1619445828231.834}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 135, "cell_type": "code", "source": "explodedGeohashedAerosolData.show(2,false)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+---------+-------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|Value         |shortName|timestamp          |Point             |index                                                                                                                                            |geohashArray1|geohash|\n+--------------+---------+-------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|1.287753193E-8|pm10     |2020-01-01 12:00:00|Point(11.3, 44.47)|[[ZOrderCurve(11.2939453125, 44.4671630859375, 11.304931640625, 44.47265625, 30, -4191437470107172864, 110001011101010100000101110101),Contains]]|[srbhcp]     |srbhcp |\n|2.210387251E-8|pm10     |2020-01-01 15:00:00|Point(11.3, 44.47)|[[ZOrderCurve(11.2939453125, 44.4671630859375, 11.304931640625, 44.47265625, 30, -4191437470107172864, 110001011101010100000101110101),Contains]]|[srbhcp]     |srbhcp |\n+--------------+---------+-------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 753.297119140625, "end_time": 1619445828997.181}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 136, "cell_type": "code", "source": "//getting plain data from CSV file (file with point Data Structure) and use UDF to get geohashes\nval geohashedMobilityData = (mobilityData\n                         .withColumn(\"index\", $\"point\" index  precision)\n                         .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedMobilityData = (geohashedMobilityData\n                                 .explode(\"geohashArray1\", \"geohash\")\n                                 { a: mutable.WrappedArray[String] => a })", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedGeohashedMobilityData: org.apache.spark.sql.DataFrame = [Code: string, Timestamp: timestamp ... 7 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 768.1279296875, "end_time": 1619445829777.805}}, "collapsed": false}}, {"execution_count": 137, "cell_type": "code", "source": "explodedGeohashedMobilityData.show(2,false)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+-------------------+-----+--------+---------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|Code    |Timestamp          |Value|Latitude|Longitude|point                |index                                                                                                                                                 |geohashArray1|geohash|\n+--------+-------------------+-----+--------+---------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|20091560|2014-10-22 12:35:37|33.0 |43.58   |13.5056  |Point(13.5056, 43.58)|[[ZOrderCurve(13.502197265625, 43.5772705078125, 13.51318359375, 43.582763671875, 30, -4191769556978499584, 110001011101001111010111110011),Contains]]|[sr9xgm]     |sr9xgm |\n|20091561|2014-10-22 12:35:37|42.0 |43.58   |13.5056  |Point(13.5056, 43.58)|[[ZOrderCurve(13.502197265625, 43.5772705078125, 13.51318359375, 43.582763671875, 30, -4191769556978499584, 110001011101001111010111110011),Contains]]|[sr9xgm]     |sr9xgm |\n+--------+-------------------+-----+--------+---------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 1264.15283203125, "end_time": 1619445831055.078}}, "collapsed": false}}, {"execution_count": 138, "cell_type": "code", "source": "val rawBologna = (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/geojson/Bologna_quartieri.geojson\")\n                  .select($\"polygon\", $\"metadata\"(\"NOMEQUART\").as(\"Neighboorhood\"))\n                  )\nval bologna = (rawBologna\n               .withColumn(\"index\", $\"polygon\" index  precision)\n               .select($\"polygon\", $\"index\", $\"Neighboorhood\")\n               .cache())\nval zorderIndexedBologna = (bologna\n                            .withColumn(\"index\", explode($\"index\"))\n                            .select(\"polygon\", \"index.curve\", \"index.relation\",\"Neighboorhood\")\n                          )\nval geohashedBologna = bologna.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedGeohashedBologna = geohashedBologna.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\nexplodedGeohashedBologna.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res36: Long = 426"}], "metadata": {"cell_status": {"execute_time": {"duration": 2269.410888671875, "end_time": 1619445833337.886}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 139, "cell_type": "code", "source": "rawBologna.show()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------+\n|             polygon| Neighboorhood|\n+--------------------+--------------+\n|magellan.Polygon@...|Borgo Panigale|\n|magellan.Polygon@...|        Navile|\n|magellan.Polygon@...|         Porto|\n|magellan.Polygon@...|          Reno|\n|magellan.Polygon@...|    San Donato|\n|magellan.Polygon@...| Santo Stefano|\n|magellan.Polygon@...|    San Vitale|\n|magellan.Polygon@...|     Saragozza|\n|magellan.Polygon@...|        Savena|\n+--------------------+--------------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 751.23681640625, "end_time": 1619445834100.838}}, "collapsed": false}}, {"execution_count": 143, "cell_type": "code", "source": "//joining geohashed trips with exploded geohashed neighborhood using filter-and-refine approach (.where($\"point\" within $\"polygon\") is refine --> using the brute force method ray casting for edge cases or false positives)\nval aerosolDataInBologna = (explodedGeohashedBologna\n                         .join(explodedGeohashedAerosolData,\n                               explodedGeohashedBologna(\"geohash\") === explodedGeohashedAerosolData(\"geohash\"))\n                         .where($\"point\" within $\"polygon\")\n                        )\naerosolDataInBologna.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+-------------+--------------------+-------+---------------+---------+-------------------+------------------+--------------------+-------------+-------+\n|             polygon|               index|Neighboorhood|        geohashArray|geohash|          Value|shortName|          timestamp|             Point|               index|geohashArray1|geohash|\n+--------------------+--------------------+-------------+--------------------+-------+---------------+---------+-------------------+------------------+--------------------+-------------+-------+\n|magellan.Polygon@...|[[ZOrderCurve(11....|    Saragozza|[srbhbd, srbhbe, ...| srbhcp|1.0493138802E-8|     pm10|2020-01-31 15:00:00|Point(11.3, 44.47)|[[ZOrderCurve(11....|     [srbhcp]| srbhcp|\n|magellan.Polygon@...|[[ZOrderCurve(11....|    Saragozza|[srbhbd, srbhbe, ...| srbhcp|1.7718040368E-8|     pm10|2020-01-31 12:00:00|Point(11.3, 44.47)|[[ZOrderCurve(11....|     [srbhcp]| srbhcp|\n|magellan.Polygon@...|[[ZOrderCurve(11....|    Saragozza|[srbhbd, srbhbe, ...| srbhcp|1.7840015687E-8|     pm10|2020-01-30 15:00:00|Point(11.3, 44.47)|[[ZOrderCurve(11....|     [srbhcp]| srbhcp|\n+--------------------+--------------------+-------------+--------------------+-------+---------------+---------+-------------------+------------------+--------------------+-------------+-------+\nonly showing top 3 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 2282.295166015625, "end_time": 1619445846403.22}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 141, "cell_type": "code", "source": "aerosolDataInBologna.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res40: Array[String] = Array(polygon, index, Neighboorhood, geohashArray, geohash, Value, shortName, timestamp, Point, index, geohashArray1, geohash)"}], "metadata": {"cell_status": {"execute_time": {"duration": 756.741943359375, "end_time": 1619445835642.494}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 142, "cell_type": "code", "source": "val mobiltyDataInBologna = (explodedGeohashedBologna\n                         .join(explodedGeohashedMobilityData,\n                               explodedGeohashedBologna(\"geohash\") === explodedGeohashedMobilityData(\"geohash\"))\n                         .where($\"point\" within $\"polygon\")\n                        )\nmobiltyDataInBologna.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+-------------+--------------------+-------+--------+-------------------+------+--------+---------+--------------------+--------------------+-------------+-------+\n|             polygon|               index|Neighboorhood|        geohashArray|geohash|    Code|          Timestamp| Value|Latitude|Longitude|               point|               index|geohashArray1|geohash|\n+--------------------+--------------------+-------------+--------------------+-------+--------+-------------------+------+--------+---------+--------------------+--------------------+-------------+-------+\n|magellan.Polygon@...|[[ZOrderCurve(11....|    Saragozza|[srbhbd, srbhbe, ...| srbj1d|20091806|2014-10-22 12:36:30|  50.0| 44.4885|  11.3227|Point(11.3227, 44...|[[ZOrderCurve(11....|     [srbj1d]| srbj1d|\n|magellan.Polygon@...|[[ZOrderCurve(11....|    Saragozza|[srbhbd, srbhbe, ...| srbj1d|20091807|2014-10-22 12:36:30|  34.5| 44.4884|  11.3226|Point(11.3226, 44...|[[ZOrderCurve(11....|     [srbj1d]| srbj1d|\n|magellan.Polygon@...|[[ZOrderCurve(11....|    Saragozza|[srbhbd, srbhbe, ...| srbj1d|20091808|2014-10-22 12:36:30|28.062| 44.4885|  11.3226|Point(11.3226, 44...|[[ZOrderCurve(11....|     [srbj1d]| srbj1d|\n+--------------------+--------------------+-------------+--------------------+-------+--------+-------------------+------+--------+---------+--------------------+--------------------+-------------+-------+\nonly showing top 3 rows"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 2277.445068359375, "end_time": 1619445837932.428}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 144, "cell_type": "code", "source": "mobiltyDataInBologna.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res44: Long = 135084"}], "metadata": {"cell_status": {"execute_time": {"duration": 3288.583984375, "end_time": 1619445889825.483}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}