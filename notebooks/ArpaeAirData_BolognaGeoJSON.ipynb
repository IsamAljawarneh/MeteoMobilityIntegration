{"nbformat_minor": 2, "cells": [{"execution_count": 17, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1619430760498_0006</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:8088/proxy/application_1619430760498_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:30060/node/containerlogs/container_1619430760498_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', u'spark.dynamicAllocation.enabled': False, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1619430760498_0006</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:8088/proxy/application_1619430760498_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-meteom.rczfdzygpd0udm3vfqzi0v2z5a.fx.internal.cloudapp.net:30060/node/containerlogs/container_1619430760498_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 785.526123046875, "end_time": 1619431956783.943}}, "collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "/**\n * @Description: a spatial join based on Filter-refine approach for NYC taxicab data\n * @author: Isam Al Jawarneh\n * @date: 02/02/2019\n *last update: 14/04/2021\n */", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 772.423828125, "end_time": 1619431957567.33}}, "collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "sc.version", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res3: String = 2.2.0.2.6.3.84-1"}], "metadata": {"cell_status": {"execute_time": {"duration": 768.06884765625, "end_time": 1619431958349.515}}, "collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.DataFrame"}], "metadata": {"cell_status": {"execute_time": {"duration": 7346.557861328125, "end_time": 1619431965711.084}}, "collapsed": false}}, {"execution_count": 55, "cell_type": "code", "source": "val containerStorageName = \"meteomobilitysparkdenis-2021-04-24t15-54-44-656z\"\nval storageAccountName = \"meteomobilityhdistorage\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "storageAccountName: String = meteomobilityhdistorage"}], "metadata": {"cell_status": {"execute_time": {"duration": 769.7099609375, "end_time": 1619433915862.994}}, "collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "/////////////////////////////\n/// Definition of schemas ///\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 765.775146484375, "end_time": 1619431966487.967}}, "collapsed": true}}, {"execution_count": 22, "cell_type": "code", "source": "val parametersRegistrySchema = StructType(Array(\n    StructField(\"IdParametro\", StringType, false),\n    StructField(\"PARAMETRO\", StringType, false),\n    StructField(\"UM\", StringType, false),\n    StructField(\"Tmed (min)\", DoubleType, false),\n    StructField(\"NOTE\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "parametersRegistrySchema: org.apache.spark.sql.types.StructType = StructType(StructField(IdParametro,StringType,false), StructField(PARAMETRO,StringType,false), StructField(UM,StringType,false), StructField(Tmed (min),DoubleType,false), StructField(NOTE,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 1292.053955078125, "end_time": 1619431967793.859}}, "collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "val stationsRegistrySchema = StructType(Array(\n    StructField(\"Stazione\", StringType, false),\n    StructField(\"Cod_staz\", StringType, false),\n    StructField(\"COMUNE\", StringType, false),\n    StructField(\"INDIRIZZO\", StringType, false),\n    StructField(\"PROVINCIA\", StringType, false),\n    StructField(\"Altezza\", StringType, false),\n    StructField(\"Id_Param\", StringType, false),\n    StructField(\"PARAMETRO\", StringType, false),\n    StructField(\"UM\", StringType, false),\n    StructField(\"Coord_X\", DoubleType, false),\n    StructField(\"Coord_Y\", DoubleType, false),\n    StructField(\"SR\", StringType, false),\n    StructField(\"LON_GEO\", DoubleType, false),\n    StructField(\"LAT_GEO\", DoubleType, false),\n    StructField(\"SR_GEO\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "stationsRegistrySchema: org.apache.spark.sql.types.StructType = StructType(StructField(Stazione,StringType,false), StructField(Cod_staz,StringType,false), StructField(COMUNE,StringType,false), StructField(INDIRIZZO,StringType,false), StructField(PROVINCIA,StringType,false), StructField(Altezza,StringType,false), StructField(Id_Param,StringType,false), StructField(PARAMETRO,StringType,false), StructField(UM,StringType,false), StructField(Coord_X,DoubleType,false), StructField(Coord_Y,DoubleType,false), StructField(SR,StringType,false), StructField(LON_GEO,DoubleType,false), StructField(LAT_GEO,DoubleType,false), StructField(SR_GEO,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 771.1669921875, "end_time": 1619431968577.239}}, "collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "val airDataSchema = StructType(Array(\n    StructField(\"COD_STAZ\", StringType, false),\n    StructField(\"ID_PARAM\", StringType, false),\n    StructField(\"DATA_INIZIO\", StringType, false),\n    StructField(\"DATA_FINE\", StringType, false),\n    StructField(\"VALORE\", DoubleType, false),\n    StructField(\"UM\", StringType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "airDataSchema: org.apache.spark.sql.types.StructType = StructType(StructField(COD_STAZ,StringType,false), StructField(ID_PARAM,StringType,false), StructField(DATA_INIZIO,StringType,false), StructField(DATA_FINE,StringType,false), StructField(VALORE,DoubleType,false), StructField(UM,StringType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 777.927978515625, "end_time": 1619431969368.135}}, "collapsed": false}}, {"execution_count": 25, "cell_type": "code", "source": "/////////////////////////////\n///// Import Dataframes /////\n/////////////////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 764.576904296875, "end_time": 1619431970144.219}}, "collapsed": true}}, {"execution_count": 29, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval parametersRegistry = (spark.read.format(\"csv\")\n                          .option(\"header\", \"true\")\n                          .schema(parametersRegistrySchema)\n                          .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/registries/Parametri.csv\")\n                          .select($\"IdParametro\".as(\"Parameter_Id\"),\n                                  $\"PARAMETRO\".as(\"Parameter_Name\"),\n                                  $\"UM\".as(\"Unit_Of_Measurement\"),\n                                  $\"Tmed (min)\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "parametersRegistry: org.apache.spark.sql.DataFrame = [Parameter_Id: string, Parameter_Name: string ... 2 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 772.22998046875, "end_time": 1619432044271.032}}, "collapsed": false}}, {"execution_count": 56, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval stationsRegistry = (spark.read.format(\"csv\")\n                        .option(\"header\", \"true\")\n                        .schema(stationsRegistrySchema)\n                        .csv(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/registries/Stazioni Aria.csv\")\n                        .select($\"Stazione\".as(\"Station\"), $\"Cod_staz\", $\"COMUNE\".as(\"City\"), $\"INDIRIZZO\".as(\"Address\"), $\"PROVINCIA\".as(\"Province\"), $\"LON_GEO\", $\"LAT_GEO\")\n                        .withColumn(\"Station_Code\", regexp_replace($\"Cod_staz\", \"\\\\.\", \"\"))\n                        .withColumn(\"Point\", point($\"LON_GEO\",$\"LAT_GEO\"))\n                        .drop(\"Cod_Staz\", \"LON_GEO\", \"LAT_GEO\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "stationsRegistry: org.apache.spark.sql.DataFrame = [Station: string, City: string ... 4 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 769.35400390625, "end_time": 1619433920637.613}}, "collapsed": false}}, {"execution_count": 57, "cell_type": "code", "source": "val airData = (spark.read.format(\"csv\")\n               .option(\"header\", \"true\")\n               .schema(airDataSchema)\n               .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/arpae_air_data\")\n               .select($\"COD_STAZ\".as(\"Station_Code\"), $\"ID_PARAM\".as(\"Parameter_Id\"), $\"DATA_INIZIO\", $\"DATA_FINE\", $\"VALORE\".as(\"Value\"))\n               .withColumn(\"Start_Date\", to_timestamp($\"DATA_INIZIO\", \"dd/MM/yyyy HH\"))\n               .withColumn(\"End_Date\", to_timestamp($\"DATA_FINE\", \"dd/MM/yyyy HH\"))\n               .drop(\"DATA_INIZIO\", \"DATA_FINE\")\n              )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "airData: org.apache.spark.sql.DataFrame = [Station_Code: string, Parameter_Id: string ... 3 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 759.89208984375, "end_time": 1619433921410.48}}, "collapsed": false}}, {"execution_count": 58, "cell_type": "code", "source": "airData.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res36: Long = 693891"}], "metadata": {"cell_status": {"execute_time": {"duration": 2273.54296875, "end_time": 1619433923697.757}}, "collapsed": false}}, {"execution_count": 59, "cell_type": "code", "source": "//////////////////\n/// Geohashing ///\n//////////////////", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 756.79296875, "end_time": 1619433924467.023}}, "collapsed": true}}, {"execution_count": 60, "cell_type": "code", "source": "// a user defined function to get geohash from long/lat point \nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@158893c4,true))))"}], "metadata": {"cell_status": {"execute_time": {"duration": 765.73193359375, "end_time": 1619433925244.926}}, "collapsed": false}}, {"execution_count": 61, "cell_type": "code", "source": "val precision = 30\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision: Int = 30"}], "metadata": {"cell_status": {"execute_time": {"duration": 754.969970703125, "end_time": 1619433926013.624}}, "collapsed": false}}, {"execution_count": 62, "cell_type": "code", "source": "//getting plain data from CSV file (file with point Data Structure) and use UDF to get geohashes\nval geohashedStations = (stationsRegistry\n                         .withColumn(\"index\", $\"point\" index  precision)\n                         .withColumn(\"geohashArray1\", geohashUDF($\"index.curve\")))\nval explodedGeohashedStations = (geohashedStations\n                                 .explode(\"geohashArray1\", \"geohash\")\n                                 { a: mutable.WrappedArray[String] => a })", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedGeohashedStations: org.apache.spark.sql.DataFrame = [Station: string, City: string ... 7 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 1267.403076171875, "end_time": 1619433927293.872}}, "collapsed": false}}, {"execution_count": 63, "cell_type": "code", "source": "explodedGeohashedStations.select(\"Station\", \"Point\", \"index\", \"geohashArray1\", \"geohash\").show(2,false)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|Station            |Point                                      |index                                                                                                                                                 |geohashArray1|geohash|\n+-------------------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|GIARDINI MARGHERITA|Point(11.35406170088398, 44.48267113876953)|[[ZOrderCurve(11.348876953125, 44.4781494140625, 11.35986328125, 44.483642578125, 30, -4191424035449470976, 110001011101010100010010000011),Contains]]|[srbj43]     |srbj43 |\n|GIARDINI MARGHERITA|Point(11.35406170088398, 44.48267113876953)|[[ZOrderCurve(11.348876953125, 44.4781494140625, 11.35986328125, 44.483642578125, 30, -4191424035449470976, 110001011101010100010010000011),Contains]]|[srbj43]     |srbj43 |\n+-------------------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 1264.386962890625, "end_time": 1619433928570.259}}, "collapsed": false}}, {"execution_count": 72, "cell_type": "code", "source": "val rawBologna = (spark.read.format(\"magellan\")\n                  .option(\"type\", \"geojson\")\n                  .load(\"wasbs://\" + containerStorageName + \"@\" + storageAccountName + \".blob.core.windows.net/data/geojson/Bologna_quartieri.geojson\")\n                  .select($\"polygon\", $\"metadata\"(\"NOMEQUART\").as(\"Neighboorhood\"))\n                  )\nval bologna = (rawBologna\n               .withColumn(\"index\", $\"polygon\" index  precision)\n               .select($\"polygon\", $\"index\", $\"Neighboorhood\")\n               .cache())\nval zorderIndexedBologna = (bologna\n                            .withColumn(\"index\", explode($\"index\"))\n                            .select(\"polygon\", \"index.curve\", \"index.relation\",\"Neighboorhood\")\n                          )\nval geohashedBologna = bologna.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\nval explodedGeohashedBologna = geohashedBologna.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\nexplodedGeohashedBologna.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res51: Long = 426"}], "metadata": {"cell_status": {"execute_time": {"duration": 2293.052001953125, "end_time": 1619434163753.934}}, "collapsed": false}}, {"execution_count": 77, "cell_type": "code", "source": "//joining geohashed trips with exploded geohashed neighborhood using filter-and-refine approach (.where($\"point\" within $\"polygon\") is refine --> using the brute force method ray casting for edge cases or false positives)\nval stationsInBologna = (explodedGeohashedBologna\n                         .join(explodedGeohashedStations,\n                               explodedGeohashedBologna(\"geohash\") === explodedGeohashedStations(\"geohash\"))\n                         .where($\"point\" within $\"polygon\")\n                        )\nstationsInBologna.select(\"point\").distinct.show(1000)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Exception in thread Thread-91:\nTraceback (most recent call last):\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/bin/anaconda/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 196, in _check_jobs\n    self._send_msgs_for_fast_job(next_job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 290, in _send_msgs_for_fast_job\n    self._send_job_start(job)\n  File \"/usr/bin/anaconda/lib/python2.7/site-packages/sparkprogressindicator/sparkmonitorbackend.py\", line 375, in _send_job_start\n    stage = stage_dict[stageId]\nKeyError: 30\n\n"}, {"output_type": "stream", "name": "stdout", "text": "+--------------------+\n|               point|\n+--------------------+\n|Point(11.35406170...|\n|Point(11.340932, ...|\n|Point(11.28508959...|\n|Point(11.32752671...|\n+--------------------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 3304.083984375, "end_time": 1619434586728.09}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}